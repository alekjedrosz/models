{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import gzip \n",
    "import os\n",
    "from typing import Tuple, Dict, Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_HOME = './data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Htj5iNRhWgz6"
   },
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    \"\"\" A logistic regression model for binary classification.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._w = None\n",
    "        self._b = None\n",
    "    \n",
    "    def fit(self, X: np.ndarray, y: np.ndarray, l_rate: float = 0.01, beta: float = 0.9, n_epochs: int = 200) -> None:\n",
    "        \"\"\" Initializes the parameters of the model and sets them using \n",
    "        batch gradient descent.\n",
    "        \n",
    "        :param np.ndarray X: A matrix of observations of shape (n_observations, n_features).\n",
    "        :param np.ndarray y: A vector of true class labels corresponding to the observations in ''X'',\n",
    "        of shape (n_observations,).\n",
    "        :param float l_rate: Learning rate to use for gradient descent.\n",
    "        :param float beta: Momentum hyperparameter.\n",
    "        :param int n_epochs: Number of training epochs to run GD for.\n",
    "        \"\"\"\n",
    "        X = X.T\n",
    "        y = np.expand_dims(y, axis=1).T\n",
    "        n_features = X.shape[0]\n",
    "        self._w = np.random.randn(n_features, 1) * np.sqrt(1 / n_features)\n",
    "        self._b = 0\n",
    "        self.GD(X, y, l_rate, beta, n_epochs)\n",
    "    \n",
    "    def GD(self, X: np.ndarray, y: np.ndarray, l_rate: float, beta: float, n_epochs: int) -> None:\n",
    "        \"\"\" Performs batch gradient descent.\n",
    "\n",
    "        :param np.ndarray X: A matrix of observations of shape (n_features, n_observations).\n",
    "        :param np.ndarray y: A matrix of true class labels corresponding to the  observations in ''X'', of shape (1, n_observations).\n",
    "        :param float l_rate: Learning rate to use for gradient descent.\n",
    "        :param float beta: Momentum hyperparameter.\n",
    "        :param int n_epochs: Number of training epochs to run GD for.\n",
    "        \"\"\"\n",
    "        m_dw = np.zeros_like(self._w)\n",
    "        m_db = 0\n",
    "        for epoch in range(n_epochs):\n",
    "            y_hat = self.forward(X)\n",
    "            dJ_dw, dJ_db = self.backward(X, y, y_hat)\n",
    "            m_dw = beta * m_dw + (1 - beta) * dJ_dw\n",
    "            m_db = beta * m_db + (1 - beta) * dJ_db\n",
    "            self._w = self._w - l_rate * m_dw\n",
    "            self._b = self._b - l_rate * m_db\n",
    "    \n",
    "    def backward(self, X: np.ndarray, y: np.ndarray, y_hat: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\" Computes the derivatives of the cost function (denoted J) wrt. the model's parameters.\n",
    "        \n",
    "        :param np.ndarray X: A numpy.ndarray of observations of shape (n_features, n_observations).\n",
    "        :param np.ndarray y: A matrix of true class labels corresponding to the \n",
    "        observations in ''X'', of shape (1, n_observations).\n",
    "        :param np.ndarray y_hat: A matrix of probabilities of the positive class predicted by the model, of\n",
    "        shape (1, n_observations).\n",
    "        :returns np.ndarray dJ_dw: The derivative of the cost function (J) wrt. to the model's weights (''self._w'').\n",
    "        :returns float dJ_db: The derivative of the cost function (J) wrt. to the model's biases (''self._b'')\n",
    "        \"\"\"\n",
    "        batch_size = X.shape[1]\n",
    "        dJ_dw = (1 / batch_size) * (X @ (y_hat - y).T)\n",
    "        dJ_db = (1 / batch_size) * np.sum(y_hat - y)\n",
    "        \n",
    "        return dJ_dw, dJ_db\n",
    "        \n",
    "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\" Computes the output of the model.\n",
    "\n",
    "        :param np.ndarray X: A matrix of observations of shape (n_features, n_observations).\n",
    "        :returns np.ndarray: A matrix of probabilities of each observation belonging to the positive\n",
    "        class, of shape (1, n_observations).\n",
    "        \"\"\"\n",
    "        return LogisticRegression.sigmoid((self._w.T @ X) + self._b)\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\" Computes predictions for a given input. Assumes the model is \n",
    "        already trained.\n",
    "        \n",
    "        :param np.ndarray X: A matrix of observations of shape (n_observations, n_features).\n",
    "        :returns np.ndarray: A vector of shape (n_observations,), indicating the predicted class for each observation.\n",
    "        \"\"\"\n",
    "        X = X.T\n",
    "        predicted_probas = self.forward(X).T\n",
    "        predictions = np.empty_like(predicted_probas)\n",
    "        predictions[predicted_probas > 0.5] = 1.\n",
    "        predictions[predicted_probas < 0.5] = 0.\n",
    "        \n",
    "        return np.squeeze(predictions)\n",
    "    \n",
    "    def random_search(self, X_train: np.ndarray, y_train: np.ndarray, X_val: np.ndarray,\n",
    "                      y_val: np.ndarray, param_grid: Dict, max_iter: int = 10, verbose: bool = False) -> Dict:\n",
    "        \"\"\" Performs random search for hyperparameter optimization.\n",
    "        \n",
    "        :param np.ndarray X_train: A matrix of train observations of shape (n_observations_train, n_features).\n",
    "        :param np.ndarray y_train: A vector of true class labels corresponding to the observations in ''X_train'',\n",
    "        of shape (n_observations_train,).\n",
    "        :param np.ndarray X_val: A matrix of validation observations of shape (n_observations_test, n_features).\n",
    "        :param np.ndarray y_val: A vector of true class labels corresponding to the observations in ''X_val'',\n",
    "        of shape (n_observations_test,).\n",
    "        :param Dict param_grid: A dictionary with hyperparameter names as keys, and tuples of (low, high) indicating\n",
    "        the range to uniformly sample from as values.\n",
    "        :param int max_iter: The number hyperparemeter of evaluations to perform.\n",
    "        :param bool verbose: If true prints the results of every evaluation to stdout.\n",
    "        :returns Dict best_hyperparameters: A dictionary of hyperparameter-value pairs, with a set of values which \n",
    "        resulted in the highest validation set score.\n",
    "        \"\"\"\n",
    "        previous_score = 0\n",
    "        best_hyperparameters = None\n",
    "        for i in range(max_iter):\n",
    "            hyperparameters = {k: np.random.uniform(v[0], v[1]) for k, v in param_grid.items()}\n",
    "            self.fit(X_train, y_train, **hyperparameters)\n",
    "            predictions = self.predict(X_val)\n",
    "            score = LogisticRegression.evaluate(y_val, predictions)\n",
    "            if verbose:\n",
    "                print(f'Score: {score}\\tHyperparemeters: {hyperparameters}')\n",
    "            if score > previous_score:\n",
    "                previous_score = score\n",
    "                best_hyperparameters = hyperparameters\n",
    "\n",
    "        return best_hyperparameters \n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(arr: np.ndarray) -> np.ndarray:\n",
    "        return 1. / (1. + np.exp(-arr))\n",
    "\n",
    "    @staticmethod\n",
    "    def evaluate(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "        # calculate accuracy\n",
    "        return np.sum(y_true == y_pred) / y_true.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_idx_data(images_fh, labels_fh):\n",
    "    \"\"\" Reads MNIST data in the idx file format from the file handles.\n",
    "    \n",
    "    :param images_fh: A file object containing MNIST images.\n",
    "    :param labels_fh: A file object containing MNIST labels.\n",
    "    :returns np.ndarray images: A matrix of unrolled images of shape (n_images, n_rows * n_cols).\n",
    "    :returns np.ndarray labels: A matrix of image labels of shape (n_images,).\n",
    "    \"\"\"\n",
    "    images_fh.seek(4)\n",
    "    labels_fh.seek(4)\n",
    "    \n",
    "    dt_uint32 = np.dtype(np.int32).newbyteorder('B')\n",
    "    n_images, n_rows, n_cols = np.frombuffer(images_fh.read(12), dtype=dt_uint32)\n",
    "    n_labels = np.frombuffer(labels_fh.read(4), dtype=dt_uint32)[0]\n",
    "    assert n_images == n_labels, 'Images and labels do not match.'\n",
    "    \n",
    "    dt_uint8 = np.dtype(np.uint8).newbyteorder('B')\n",
    "    images_buffer = images_fh.read(n_images * n_rows * n_cols)\n",
    "    images = np.frombuffer(images_buffer, dtype=dt_uint8).astype(np.float64)\n",
    "    images = images.reshape(n_images, n_rows*n_cols)\n",
    "    \n",
    "    labels_buffer = labels_fh.read(n_labels)\n",
    "    labels = np.frombuffer(labels_buffer, dtype=dt_uint8).astype(np.float64)\n",
    "    \n",
    "    return images, labels\n",
    "\n",
    "def preprocess_data(images, labels):\n",
    "    \"\"\" Excludes MNIST images with labels in (0, 1) and assigns a binary label to the \n",
    "    remaining images, where prime numbers are the positive class (1) and the remaining \n",
    "    numbers are the negative class (0).\n",
    "    \n",
    "    :param np.ndarray images: A matrix of shape (n_images, n_rows * n_cols) containing \n",
    "    the MNIST images.\n",
    "    :param np.ndarray labels: A matrix of shape (n_images,) containing the MNIST labels.\n",
    "    :returns np.ndarray images: A reduced matrix of images of shape (n_images, n_rows * n_cols).\n",
    "    :returns np.ndarray labels: An adjusted matrix of image labels of shape (n_images,).\n",
    "    \"\"\"\n",
    "    mask = ~np.isin(labels, (0, 1))\n",
    "    labels = labels[mask]\n",
    "    images = images[mask, :]\n",
    "    labels[np.isin(labels, (2, 3, 5, 7))] = 1\n",
    "    labels[np.isin(labels, (4, 6, 8, 9))] = 0\n",
    "    shuffled_index = np.random.permutation(labels.shape[0])\n",
    "    images = images[shuffled_index, :]\n",
    "    labels = labels[shuffled_index]\n",
    "    \n",
    "    return images, labels\n",
    "\n",
    "def standardize_data(input_data, m=None, s=None):\n",
    "    \"\"\" Standardizes the input data along the first axis, to zero mean\n",
    "    and unit variance.\n",
    "    \n",
    "    :param np.ndarray input_data: Data to standardize, of shape (n_observations, n_features).\n",
    "    :param np.ndarray m: The mean to use for standardizing each of the columns, of shape\n",
    "    (1, n_features).\n",
    "    :param np.ndarray s: The standard deviation to use for standardizing each of the columns, \n",
    "    of shape (1, n_features).\n",
    "    :returns np.ndarray input_data: Standardized input data, of shape (n_observations, n_features).\n",
    "    :returns m: Means used for standardizing each of the columns, of shape (1, n_features).\n",
    "    :returns s: Standard deviations used for standardizing each of the columns, of shape (1, n_features).\n",
    "    \"\"\"\n",
    "    if m is None:\n",
    "        m = np.mean(input_data, axis=0)\n",
    "    if s is None:\n",
    "        s = np.std(input_data, axis=0, ddof=1)\n",
    "        s[s == 0] = 1\n",
    "    input_data = (input_data - m) / s\n",
    "    return input_data, m, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gdXcw0IZWlZ_"
   },
   "outputs": [],
   "source": [
    "with gzip.open(os.path.join(DATA_HOME, 'train-images-idx3-ubyte.gz'), 'r') as train_images_fh, \\\n",
    "     gzip.open(os.path.join(DATA_HOME, 'train-labels-idx1-ubyte.gz'), 'r') as train_labels_fh, \\\n",
    "     gzip.open(os.path.join(DATA_HOME, 't10k-images-idx3-ubyte.gz'), 'r') as validation_images_fh, \\\n",
    "     gzip.open(os.path.join(DATA_HOME, 't10k-labels-idx1-ubyte.gz'), 'r') as validation_labels_fh:\n",
    "    \n",
    "    train_images, train_labels = read_idx_data(train_images_fh, train_labels_fh)\n",
    "    validation_images, validation_labels = read_idx_data(validation_images_fh, validation_labels_fh)\n",
    "    \n",
    "train_images, train_labels = preprocess_data(train_images, train_labels)\n",
    "validation_images, validation_labels = preprocess_data(validation_images, validation_labels)\n",
    "train_images, train_means, train_stds = standardize_data(train_images)\n",
    "validation_images, _, _ = standardize_data(validation_images, m=train_means, s=train_stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'l_rate': (5, 25), 'beta': (0.9, 0.91)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.9156626506024096\tHyperparemeters: {'l_rate': 19.993934282815687, 'beta': 0.9036566642914767}\n",
      "Score: 0.9222574508560558\tHyperparemeters: {'l_rate': 6.294814438877758, 'beta': 0.9080081054705631}\n",
      "Score: 0.9223842739378567\tHyperparemeters: {'l_rate': 12.794830859252762, 'beta': 0.9074435926918728}\n",
      "Score: 0.8843373493975903\tHyperparemeters: {'l_rate': 23.162074809523997, 'beta': 0.9089540526367388}\n",
      "Score: 0.922003804692454\tHyperparemeters: {'l_rate': 9.334554216616693, 'beta': 0.9050502634341551}\n",
      "Score: 0.9225110970196576\tHyperparemeters: {'l_rate': 14.940045178638762, 'beta': 0.9071430613239186}\n",
      "Score: 0.922003804692454\tHyperparemeters: {'l_rate': 5.858018570214243, 'beta': 0.9023724935285407}\n",
      "Score: 0.9223842739378567\tHyperparemeters: {'l_rate': 12.16275164043645, 'beta': 0.9091266207572261}\n",
      "Score: 0.9222574508560558\tHyperparemeters: {'l_rate': 13.997005108589311, 'beta': 0.9035348020563608}\n",
      "Score: 0.9222574508560558\tHyperparemeters: {'l_rate': 18.802007153326038, 'beta': 0.9067867491103272}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'l_rate': 14.940045178638762, 'beta': 0.9071430613239186}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.random_search(train_images, train_labels, validation_images, validation_labels, param_grid, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'l_rate': (10, 15), 'beta': (0.7, 0.99)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.9109701965757768\tHyperparemeters: {'l_rate': 14.576844242956494, 'beta': 0.8161911894115483}\n",
      "Score: 0.8961318960050729\tHyperparemeters: {'l_rate': 13.862720697412245, 'beta': 0.763297032968308}\n",
      "Score: 0.9221306277742549\tHyperparemeters: {'l_rate': 10.422989884187569, 'beta': 0.9411020415748607}\n",
      "Score: 0.9113506658211794\tHyperparemeters: {'l_rate': 14.293866683574208, 'beta': 0.9879432846543625}\n",
      "Score: 0.8882688649334178\tHyperparemeters: {'l_rate': 11.567523595612396, 'beta': 0.8046831591624788}\n",
      "Score: 0.9228915662650602\tHyperparemeters: {'l_rate': 13.23824695611749, 'beta': 0.9636095984547395}\n",
      "Score: 0.9223842739378567\tHyperparemeters: {'l_rate': 11.367521550301937, 'beta': 0.8549928407320002}\n",
      "Score: 0.9214965123652504\tHyperparemeters: {'l_rate': 14.209799741359738, 'beta': 0.8698640208779702}\n",
      "Score: 0.9223842739378567\tHyperparemeters: {'l_rate': 14.52944031479775, 'beta': 0.9281576492622502}\n",
      "Score: 0.8838300570703869\tHyperparemeters: {'l_rate': 10.56071119199618, 'beta': 0.8093319236200329}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'l_rate': 13.23824695611749, 'beta': 0.9636095984547395}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.random_search(train_images, train_labels, validation_images, validation_labels, param_grid, verbose=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Research_Engineer_Warszawa_i_Krak√≥w",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
