{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import gzip \n",
    "import os\n",
    "from typing import Tuple, Dict, Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_HOME = './data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Htj5iNRhWgz6"
   },
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    \"\"\" A logistic regression model for binary classification.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._w = None\n",
    "        self._b = None\n",
    "    \n",
    "    def fit(self, X: np.ndarray, y: np.ndarray, l_rate: float = 0.01, beta: float = 0.9,\n",
    "            n_epochs: int = 200) -> None:\n",
    "        \"\"\" Initializes the parameters of the model and sets them using \n",
    "        batch gradient descent.\n",
    "        \n",
    "        :param np.ndarray X: A matrix of observations of shape (n_observations, n_features).\n",
    "        :param np.ndarray y: A vector of true class labels corresponding to the observations in ''X'',\n",
    "        of shape (n_observations,).\n",
    "        :param float l_rate: Learning rate to use for gradient descent.\n",
    "        :param float beta: Momentum hyperparameter.\n",
    "        :param int n_epochs: Number of training epochs to run GD for.\n",
    "        \"\"\"\n",
    "        X = X.T\n",
    "        y = np.expand_dims(y, axis=1).T\n",
    "        n_features = X.shape[0]\n",
    "        self._w = np.random.randn(n_features, 1) * np.sqrt(1 / n_features)\n",
    "        self._b = 0\n",
    "        self.GD(X, y, l_rate, beta, n_epochs)\n",
    "    \n",
    "    def GD(self, X: np.ndarray, y: np.ndarray, l_rate: float, beta: float, n_epochs: int) -> None:\n",
    "        \"\"\" Performs batch gradient descent.\n",
    "\n",
    "        :param np.ndarray X: A matrix of observations of shape (n_features, n_observations).\n",
    "        :param np.ndarray y: A matrix of true class labels corresponding to the  observations\n",
    "        in ''X'', of shape (1, n_observations).\n",
    "        :param float l_rate: Learning rate to use for gradient descent.\n",
    "        :param float beta: Momentum hyperparameter.\n",
    "        :param int n_epochs: Number of training epochs to run GD for.\n",
    "        \"\"\"\n",
    "        m_dw = np.zeros_like(self._w)\n",
    "        m_db = 0\n",
    "        for epoch in range(n_epochs):\n",
    "            y_hat = self.forward(X)\n",
    "            dJ_dw, dJ_db = self.backward(X, y, y_hat)\n",
    "            m_dw = beta * m_dw + (1 - beta) * dJ_dw\n",
    "            m_db = beta * m_db + (1 - beta) * dJ_db\n",
    "            self._w = self._w - l_rate * m_dw\n",
    "            self._b = self._b - l_rate * m_db\n",
    "    \n",
    "    def backward(self, X: np.ndarray, y: np.ndarray, y_hat: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\" Computes the derivatives of the cost function (denoted J) wrt. the model's parameters.\n",
    "        \n",
    "        :param np.ndarray X: A numpy.ndarray of observations of shape (n_features, n_observations).\n",
    "        :param np.ndarray y: A matrix of true class labels corresponding to the \n",
    "        observations in ''X'', of shape (1, n_observations).\n",
    "        :param np.ndarray y_hat: A matrix of probabilities of the positive class predicted by the model,\n",
    "        of shape (1, n_observations).\n",
    "        :returns np.ndarray dJ_dw: The derivative of the cost function (J) wrt. to the model's\n",
    "        weights (''self._w'').\n",
    "        :returns float dJ_db: The derivative of the cost function (J) wrt. to the model's \n",
    "        biases (''self._b'')\n",
    "        \"\"\"\n",
    "        batch_size = X.shape[1]\n",
    "        dJ_dw = (1 / batch_size) * (X @ (y_hat - y).T)\n",
    "        dJ_db = (1 / batch_size) * np.sum(y_hat - y)\n",
    "        \n",
    "        return dJ_dw, dJ_db\n",
    "        \n",
    "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\" Computes the output of the model.\n",
    "\n",
    "        :param np.ndarray X: A matrix of observations of shape (n_features, n_observations).\n",
    "        :returns np.ndarray: A matrix of probabilities of each observation belonging to the positive\n",
    "        class, of shape (1, n_observations).\n",
    "        \"\"\"\n",
    "        return LogisticRegression.sigmoid((self._w.T @ X) + self._b)\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\" Computes predictions for a given input. Assumes the model is \n",
    "        already trained.\n",
    "        \n",
    "        :param np.ndarray X: A matrix of observations of shape (n_observations, n_features).\n",
    "        :returns np.ndarray: A vector of shape (n_observations,), indicating the predicted class \n",
    "        for each observation.\n",
    "        \"\"\"\n",
    "        X = X.T\n",
    "        predicted_probas = self.forward(X).T\n",
    "        predictions = np.empty_like(predicted_probas)\n",
    "        predictions[predicted_probas > 0.5] = 1.\n",
    "        predictions[predicted_probas < 0.5] = 0.\n",
    "        \n",
    "        return np.squeeze(predictions)\n",
    "    \n",
    "    def random_search(self, X_train: np.ndarray, y_train: np.ndarray, X_val: np.ndarray,\n",
    "                      y_val: np.ndarray, param_grid: Dict, max_iter: int = 10, verbose: bool = False) -> Dict:\n",
    "        \"\"\" Performs random search for hyperparameter optimization.\n",
    "        \n",
    "        :param np.ndarray X_train: A matrix of train observations of shape \n",
    "        (n_observations_train, n_features).\n",
    "        :param np.ndarray y_train: A vector of true class labels corresponding to the observations \n",
    "        in ''X_train'', of shape (n_observations_train,).\n",
    "        :param np.ndarray X_val: A matrix of validation observations of shape \n",
    "        (n_observations_test, n_features).\n",
    "        :param np.ndarray y_val: A vector of true class labels corresponding to the observations\n",
    "        in ''X_val'', of shape (n_observations_test,).\n",
    "        :param Dict param_grid: A dictionary with hyperparameter names as keys, and tuples of\n",
    "        (low, high) indicating the range to uniformly sample from as values.\n",
    "        :param int max_iter: The number hyperparemeter of evaluations to perform.\n",
    "        :param bool verbose: If true prints the results of every evaluation to stdout.\n",
    "        :returns Dict best_hyperparameters: A dictionary of hyperparameter-value pairs, with a \n",
    "        set of values which resulted in the highest validation set score.\n",
    "        \"\"\"\n",
    "        previous_score = 0\n",
    "        best_hyperparameters = None\n",
    "        for i in range(max_iter):\n",
    "            hyperparameters = {k: np.random.uniform(v[0], v[1]) for k, v in param_grid.items()}\n",
    "            self.fit(X_train, y_train, **hyperparameters)\n",
    "            predictions = self.predict(X_val)\n",
    "            score = LogisticRegression.evaluate(y_val, predictions)\n",
    "            if verbose:\n",
    "                print(f'Score: {score}\\tHyperparemeters: {hyperparameters}')\n",
    "            if score > previous_score:\n",
    "                previous_score = score\n",
    "                best_hyperparameters = hyperparameters\n",
    "\n",
    "        return best_hyperparameters \n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(arr: np.ndarray) -> np.ndarray:\n",
    "        return 1. / (1. + np.exp(-arr))\n",
    "\n",
    "    @staticmethod\n",
    "    def evaluate(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "        # calculate accuracy\n",
    "        return np.sum(y_true == y_pred) / y_true.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_idx_data(images_fh, labels_fh):\n",
    "    \"\"\" Reads MNIST data in the idx file format from the file handles.\n",
    "    \n",
    "    :param images_fh: A file object containing MNIST images.\n",
    "    :param labels_fh: A file object containing MNIST labels.\n",
    "    :returns np.ndarray images: A matrix of unrolled images of shape (n_images, n_rows * n_cols).\n",
    "    :returns np.ndarray labels: A matrix of image labels of shape (n_images,).\n",
    "    \"\"\"\n",
    "    images_fh.seek(4)\n",
    "    labels_fh.seek(4)\n",
    "    \n",
    "    dt_uint32 = np.dtype(np.int32).newbyteorder('B')\n",
    "    n_images, n_rows, n_cols = np.frombuffer(images_fh.read(12), dtype=dt_uint32)\n",
    "    n_labels = np.frombuffer(labels_fh.read(4), dtype=dt_uint32)[0]\n",
    "    assert n_images == n_labels, 'Images and labels do not match.'\n",
    "    \n",
    "    dt_uint8 = np.dtype(np.uint8).newbyteorder('B')\n",
    "    images_buffer = images_fh.read(n_images * n_rows * n_cols)\n",
    "    images = np.frombuffer(images_buffer, dtype=dt_uint8).astype(np.float64)\n",
    "    images = images.reshape(n_images, n_rows*n_cols)\n",
    "    \n",
    "    labels_buffer = labels_fh.read(n_labels)\n",
    "    labels = np.frombuffer(labels_buffer, dtype=dt_uint8).astype(np.float64)\n",
    "    \n",
    "    return images, labels\n",
    "\n",
    "def preprocess_data(images, labels):\n",
    "    \"\"\" Excludes MNIST images with labels in (0, 1) and assigns a binary label to the \n",
    "    remaining images, where prime numbers are the positive class (1) and the remaining \n",
    "    numbers are the negative class (0).\n",
    "    \n",
    "    :param np.ndarray images: A matrix of shape (n_images, n_rows * n_cols) containing \n",
    "    the MNIST images.\n",
    "    :param np.ndarray labels: A matrix of shape (n_images,) containing the MNIST labels.\n",
    "    :returns np.ndarray images: A reduced matrix of images of shape (n_images, n_rows * n_cols).\n",
    "    :returns np.ndarray labels: An adjusted matrix of image labels of shape (n_images,).\n",
    "    \"\"\"\n",
    "    mask = ~np.isin(labels, (0, 1))\n",
    "    labels = labels[mask]\n",
    "    images = images[mask, :]\n",
    "    labels[np.isin(labels, (2, 3, 5, 7))] = 1\n",
    "    labels[np.isin(labels, (4, 6, 8, 9))] = 0\n",
    "    shuffled_index = np.random.permutation(labels.shape[0])\n",
    "    images = images[shuffled_index, :]\n",
    "    labels = labels[shuffled_index]\n",
    "    \n",
    "    return images, labels\n",
    "\n",
    "def standardize_data(input_data, m=None, s=None):\n",
    "    \"\"\" Standardizes the input data along the first axis, to zero mean\n",
    "    and unit variance.\n",
    "    \n",
    "    :param np.ndarray input_data: Data to standardize, of shape (n_observations, n_features).\n",
    "    :param np.ndarray m: The mean to use for standardizing each of the columns, of shape\n",
    "    (1, n_features).\n",
    "    :param np.ndarray s: The standard deviation to use for standardizing each of the columns, \n",
    "    of shape (1, n_features).\n",
    "    :returns np.ndarray input_data: Standardized input data, of shape (n_observations, n_features).\n",
    "    :returns m: Means used for standardizing each of the columns, of shape (1, n_features).\n",
    "    :returns s: Standard deviations used for standardizing each of the columns, of shape (1, n_features).\n",
    "    \"\"\"\n",
    "    if m is None:\n",
    "        m = np.mean(input_data, axis=0)\n",
    "    if s is None:\n",
    "        s = np.std(input_data, axis=0, ddof=1)\n",
    "        s[s == 0] = 1\n",
    "    input_data = (input_data - m) / s\n",
    "    return input_data, m, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gdXcw0IZWlZ_"
   },
   "outputs": [],
   "source": [
    "with gzip.open(os.path.join(DATA_HOME, 'train-images-idx3-ubyte.gz'), 'r') as train_images_fh, \\\n",
    "     gzip.open(os.path.join(DATA_HOME, 'train-labels-idx1-ubyte.gz'), 'r') as train_labels_fh, \\\n",
    "     gzip.open(os.path.join(DATA_HOME, 't10k-images-idx3-ubyte.gz'), 'r') as validation_images_fh, \\\n",
    "     gzip.open(os.path.join(DATA_HOME, 't10k-labels-idx1-ubyte.gz'), 'r') as validation_labels_fh:\n",
    "    \n",
    "    train_images, train_labels = read_idx_data(train_images_fh, train_labels_fh)\n",
    "    validation_images, validation_labels = read_idx_data(validation_images_fh, validation_labels_fh)\n",
    "    \n",
    "train_images, train_labels = preprocess_data(train_images, train_labels)\n",
    "validation_images, validation_labels = preprocess_data(validation_images, validation_labels)\n",
    "train_images, train_means, train_stds = standardize_data(train_images)\n",
    "validation_images, _, _ = standardize_data(validation_images, m=train_means, s=train_stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'l_rate': (5, 25), 'beta': (0.9, 0.91)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.9083069118579582\tHyperparemeters: {'l_rate': 20.446831714748747, 'beta': 0.9063223176444059}\n",
      "Score: 0.9004438807863031\tHyperparemeters: {'l_rate': 22.898759468142742, 'beta': 0.9070570283155779}\n",
      "Score: 0.9099556119213696\tHyperparemeters: {'l_rate': 19.299737767449678, 'beta': 0.900403946906465}\n",
      "Score: 0.9223842739378567\tHyperparemeters: {'l_rate': 12.919202803777123, 'beta': 0.9016883480222472}\n",
      "Score: 0.922003804692454\tHyperparemeters: {'l_rate': 7.8997811843698145, 'beta': 0.9057628094956582}\n",
      "Score: 0.9008243500317058\tHyperparemeters: {'l_rate': 23.97190546161388, 'beta': 0.908652282986884}\n",
      "Score: 0.922003804692454\tHyperparemeters: {'l_rate': 5.940688811954455, 'beta': 0.9066727333506089}\n",
      "Score: 0.9223842739378567\tHyperparemeters: {'l_rate': 12.523358158650241, 'beta': 0.9088677963385705}\n",
      "Score: 0.9225110970196576\tHyperparemeters: {'l_rate': 18.50672405708613, 'beta': 0.9071609332550347}\n",
      "Score: 0.9218769816106531\tHyperparemeters: {'l_rate': 21.784958308754746, 'beta': 0.9091956383131162}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'l_rate': 18.50672405708613, 'beta': 0.9071609332550347}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.random_search(train_images, train_labels, validation_images, validation_labels, param_grid, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'l_rate': (10, 15), 'beta': (0.7, 0.99)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.9225110970196576\tHyperparemeters: {'l_rate': 13.602435460729332, 'beta': 0.9236367196357657}\n",
      "Score: 0.7941661382371592\tHyperparemeters: {'l_rate': 13.908523622664736, 'beta': 0.7002938046855118}\n",
      "Score: 0.9223842739378567\tHyperparemeters: {'l_rate': 12.141051496891606, 'beta': 0.9148278709704178}\n",
      "Score: 0.9131261889663919\tHyperparemeters: {'l_rate': 10.721843686059321, 'beta': 0.742406135929706}\n",
      "Score: 0.9098287888395687\tHyperparemeters: {'l_rate': 13.439493075604547, 'beta': 0.7946355992848368}\n",
      "Score: 0.9029803424223208\tHyperparemeters: {'l_rate': 13.944524446356372, 'beta': 0.7054191069281142}\n",
      "Score: 0.9160431198478123\tHyperparemeters: {'l_rate': 10.98545984402508, 'beta': 0.7521296683347183}\n",
      "Score: 0.8995561192136969\tHyperparemeters: {'l_rate': 14.955882552187191, 'beta': 0.8131863065957504}\n",
      "Score: 0.9218769816106531\tHyperparemeters: {'l_rate': 12.976937157145663, 'beta': 0.945632204942269}\n",
      "Score: 0.8852251109701966\tHyperparemeters: {'l_rate': 14.23806858035868, 'beta': 0.718507323649393}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'l_rate': 13.602435460729332, 'beta': 0.9236367196357657}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.random_search(train_images, train_labels, validation_images, validation_labels, param_grid, verbose=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Research_Engineer_Warszawa_i_Krak√≥w",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
